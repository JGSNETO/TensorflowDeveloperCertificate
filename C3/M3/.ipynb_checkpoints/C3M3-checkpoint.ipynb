{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35c9246a-46a8-498d-a38c-0aa495729cab",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTMs)\n",
    "\n",
    "- So for a neural network, to take into account the ordering of the words, people now use specialized Neural Network Architectures, things like an RNN, or GIO, or LSTM\n",
    "\n",
    "- LSTM (Long Short-Term Memory) is a type of recurrent neural network (RNN) architecture that excels at processing sequential data by addressing the issue of long-term dependencies. In natural language recognition tasks like text classification, language modeling, sentiment analysis, machine translation, and speech recognition, LSTM models have become crucial due to their ability to remember and utilize information over long sequences of words.\n",
    "\n",
    "Here’s an in-depth explanation of how LSTM works within the context of natural language recognition using TensorFlow:\n",
    "\n",
    "## LSTM: Basics and Structure\n",
    "\n",
    "LSTMs are designed to handle long-term dependencies in data sequences by maintaining a memory cell. In NLP tasks, this is useful because earlier words in a sentence often influence the meaning of later words.\n",
    "\n",
    "LSTM cells have the following key components:\n",
    "\n",
    "Cell state: This is the memory of the network, which retains long-term information.\n",
    "Hidden state: This represents the short-term memory, which is used to make predictions or pass information between time steps.\n",
    "Input gate: Controls how much of the current input should be added to the memory.\n",
    "Forget gate: Decides how much of the previous memory should be forgotten.\n",
    "Output gate: Determines what part of the memory should be output to the next step.\n",
    "The mathematical formulation for an LSTM cell involves several steps, mainly managing the memory and updating it using the input, forget, and output gates.\n",
    "\n",
    "## Why LSTM is useful in NLP\n",
    "\n",
    "- Handling Sequential Data: Sentences are sequential, where each word is related to the previous ones. LSTMs capture these relationships.\n",
    "- Addressing Vanishing Gradient Problem: Traditional RNNs struggle with vanishing gradients, making them forget dependencies over long sequences. LSTMs overcome this by using gating mechanisms.\n",
    "- Context Awareness: LSTMs maintain context over long input sequences, crucial for tasks like speech recognition or understanding complex sentences where distant words affect meaning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5902ca8-88cc-4d82-9ec8-8b8762b3f654",
   "metadata": {},
   "source": [
    "# Using LSTMs in Tensorflow for NLP tasks\n",
    "\n",
    "- TensorFlow provides high-level APIs like Keras, which makes it easy to implement LSTMs for natural language tasks. Below is a simplified example of using LSTMs in TensorFlow for a natural language recognition task like text classification.\n",
    "\n",
    "## Step 1: Data Preparation\n",
    "\n",
    "- For natural language tasks, text data is often preprocessed using tokenization and padding, where:\n",
    "\n",
    "1. Tokenization: Breaks down sentences into words or subwords and converts them into integer sequences.\n",
    "2. Padding: Ensures that all input sequences have the same length by padding shorter sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71c9b4af-3eed-49e1-895d-2d3e7eeb2699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\"I love natural language processing\", \"LSTMs are great for sequential data\"]\n",
    "\n",
    "# Tokenizing the sentences\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Padding the sequences to make them the same length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0db1c7a-b881-4df4-8e22-a70ff076669f",
   "metadata": {},
   "source": [
    "# Step 2: Building the LSTM Model\n",
    "\n",
    "Now, let’s define an LSTM model for text classification (for example, sentiment analysis). This model will take in sequences of words (in integer format), process them using an embedding layer, and then pass through an LSTM layer before making a prediction.\n",
    "\n",
    "1. Embedding Layer: Converts each word (represented by an integer) into dense vectors of a fixed size. These vectors capture the semantic meaning of words.\n",
    "2. LSTM Layer: Processes the sequences and learns dependencies between words.\n",
    "3. Dense Layer: Used for the final classification, with a sigmoid activation function for binary classification tasks (e.g., positive/negative sentiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dffe112-4a8e-4d9e-8421-ff40988444c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 10, 128)           1280000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               131584    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1411713 (5.39 MB)\n",
      "Trainable params: 1411713 (5.39 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer to learn word representations\n",
    "model.add(Embedding(input_dim=10000, output_dim=128, input_length=10))\n",
    "\n",
    "# Add an LSTM layer\n",
    "model.add(LSTM(units=128))\n",
    "\n",
    "# Add a Dense layer for classification\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764e405b-72b9-4c74-bd55-f691017ca8b3",
   "metadata": {},
   "source": [
    "# Step 3: Training the Model\n",
    "\n",
    "- Once the model is built, it can be trained on labeled text data using standard TensorFlow/Keras training procedures.\n",
    "- During training, the model learns to associate patterns in sequences of words with output labels (e.g., positive/negative sentiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a7dfb7c-3e5b-49b8-ae69-f07fd6f24c45",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming you have prepared your training data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# X_train: padded sequences, y_train: labels\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming you have prepared your training data\n",
    "# X_train: padded sequences, y_train: labels\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e51de0-dd09-435f-9cbd-75c4d6907969",
   "metadata": {},
   "source": [
    "# Advantages of LSTM in NLP with Tensorflow\n",
    "\n",
    "\n",
    "1. Better Context Retention: Compared to traditional RNNs, LSTMs can maintain long-term dependencies, which is crucial in language tasks.\n",
    "2. Easy to Implement: TensorFlow’s Keras API makes it easy to implement and train LSTM models on large-scale NLP tasks.\n",
    "3. Support for Complex Tasks: LSTMs are suitable for complex language tasks like question answering, speech recognition, and translation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
