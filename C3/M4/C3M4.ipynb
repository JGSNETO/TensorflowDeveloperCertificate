{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "\n",
    "- One of the most fun applications of sequence models, is that they can read the body of text, so train on the certain body of text, and then generate or synthesize new texts, that sounds like it was written by similar author or set of authors.\n",
    "- Instead of generating new text, how about thinking about it as a prediction problem.\n",
    "- We can get a body of text, extract the full vocabulary from it, and then create datasets from that where we make a phrase the x's and the next word in that phrase to be the y's\n",
    "- Text generation in TensorFlow involves using deep learning models to generate sequences of text. It typically relies on neural networks trained to predict the next word or character in a sequence based on previous inputs. The general approach involves using models like Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTMs), or more recently, Transformer-based models, such as GPT or BERT, implemented in TensorFlow.\n",
    "\n",
    "Hereâ€™s how text generation in TensorFlow generally works:\n",
    "\n",
    "1. Dataset Preparation\n",
    "Text Corpus: First, you need a large text corpus for training. The text is often preprocessed to remove unwanted characters, tokenize words or characters, and convert them to a numerical representation (e.g., integer sequences).\n",
    "Tokenization: The text is split into tokens, which can be words or characters, depending on the granularity of the model. In TensorFlow, this can be done using the Tokenizer class from tensorflow.keras.preprocessing.text.\n",
    "2. Text to Sequences\n",
    "The tokenizer converts the text into sequences of integers, where each word or character is mapped to a unique integer.\n",
    "Example: \"I love cats\" -> [5, 22, 13].\n",
    "3. Creating Input and Output Sequences\n",
    "The model is trained to predict the next word/character in a sequence.\n",
    "For example, for the sequence \"I love cats\":\n",
    "Input: \"I love\", Output: \"cats\".\n",
    "These sequences are often created using a sliding window approach to generate multiple input-output pairs from a single text corpus.\n",
    "4. Model Building\n",
    "You can use different models in TensorFlow for text generation. The key ones are:\n",
    "\n",
    "(a) Recurrent Neural Networks (RNN)\n",
    "RNNs process sequences of data by maintaining a hidden state that captures information about previous elements in the sequence.\n",
    "TensorFlow makes it easy to build RNNs using tf.keras.layers.SimpleRNN.\n",
    "(b) Long Short-Term Memory Networks (LSTM)\n",
    "LSTMs are a special type of RNN designed to overcome the vanishing gradient problem, which allows them to capture longer-term dependencies in text.\n",
    "TensorFlow provides LSTM layers through tf.keras.layers.LSTM.\n",
    "(c) Gated Recurrent Units (GRUs)\n",
    "GRUs are similar to LSTMs but have fewer gates and parameters, making them slightly faster but less expressive.\n",
    "In TensorFlow, GRUs are implemented with tf.keras.layers.GRU.\n",
    "(d) Transformer Models\n",
    "Transformer-based architectures (like GPT, BERT) are state-of-the-art for text generation tasks. They use self-attention mechanisms to model relationships between words across a sequence without relying on recurrent connections.\n",
    "TensorFlow has implementations like tf.keras.layers.MultiHeadAttention for building custom Transformer-based models or can leverage pre-built models like GPT-2 from TensorFlow Hub.\n",
    "5. Training the Model\n",
    "The model is trained to minimize the loss (often categorical cross-entropy) between the predicted token and the actual token that should follow in the sequence.\n",
    "Example: If the model is given the input \"I love\" and it predicts \"dogs\" instead of \"cats,\" the loss will be high, and the model will update its weights through backpropagation.\n",
    "6. Generating Text (Inference)\n",
    "After training, the model can generate new text by feeding it a starting seed sequence and predicting one token at a time, appending the predicted token to the sequence, and then feeding the updated sequence back into the model to predict the next token. This process continues until the desired text length is reached or an end token is generated.\n",
    "\n",
    "There are various strategies for generating text:\n",
    "\n",
    "Greedy Search: Always choose the word with the highest probability as the next word.\n",
    "Beam Search: Keep track of multiple candidate sequences and choose the sequence with the highest overall probability.\n",
    "Sampling: Randomly sample from the probability distribution of the predicted words to introduce more diversity.\n",
    "Temperature Control: Adjusts the randomness of predictions by scaling the logits. A higher temperature leads to more randomness, while a lower temperature makes the model more conservative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Sample text data\n",
    "text = \"I love TensorFlow. It is a great framework for deep learning.\"\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "sequences = tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "# Create input-output pairs\n",
    "input_sequences = []\n",
    "for i in range(1, len(sequences)):\n",
    "    input_sequences.append(sequences[:i+1])\n",
    "\n",
    "# Pad sequences to ensure they are of the same length\n",
    "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences)\n",
    "\n",
    "# Split into input (X) and output (y)\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "\n",
    "# One-hot encode output\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=len(tokenizer.word_index) + 1)\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, 50, input_length=X.shape[1]),\n",
    "    tf.keras.layers.LSTM(100),\n",
    "    tf.keras.layers.Dense(len(tokenizer.word_index) + 1, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=100, verbose=2)\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(seed_text, next_words, model, tokenizer):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], maxlen=X.shape[1], padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "        output_word = tokenizer.index_word[predicted[0]]\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "# Generate text based on seed\n",
    "print(generate_text(\"I love\", 5, model, tokenizer))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
