{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda Layer\n",
    "\n",
    "- Lambda layer is a flexible, user-defined layer that allows you to apply custom operations to the input of the layer. It is often used to implement simple or complex functions without needing to define a full custom layer class. \n",
    "\n",
    "## Key features of a Lambda Layer\n",
    "\n",
    "1. Custom operations\n",
    "- A lambda layer can be used to apply mathematical operations, transformations, or arbitrary functions to the input tensot. \n",
    "2. Inline definition\n",
    "- You can define the function directly inline as a python lambda function or as a named python function. \n",
    "3. No parameters\n",
    "- Unlike dense, convolutional, or recurrent layers, a Lambda layer typically does not have trainable parameters.\n",
    "4. Flexibility:\n",
    "- It is ideal for tasks like slicing, reshaping, scaling, combining inouts or applying element wise functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Scaling Input Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential([\n",
    "    Lambda(lambda x: x / 255.0, input_shape=(28, 28, 1))  # Normalizing pixel values\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Tensor Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Lambda(lambda x: x ** 2))  # Square each element in the input tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding or Modifying Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Lambda(lambda x: x[:, :, ::-1]))  # Flip the tensor along the last dimension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN)\n",
    "\n",
    "- A Recurrent Neural Network, or RNN is a neural network that contains recurrent layers.\n",
    "- RNN is a type of neural network designed to process sequential data by using loops to allow information to persist accros time steps. Unlike traditional neural networks, RNN have a hidden state that remembers information from previous inputs, making them ideal for tasks like time series prediction, language modeling, and speech recognition. They process inputs sequentially, step by step, using the same weights accross all steps. However, they can struggle with long-term dependencies due to issues like vanish gradients. Variants like LSTMs(Long short-term memory) and GRUs(Gated Recurrent Units) address these languages. \n",
    "\n",
    "## Key features\n",
    "\n",
    "1. Hidden state: RNNs maintain a hidden state that gets updated at each time step. This hidden state acts as a memory, storing information about previous elements in the sequence. \n",
    "2. Shared weights: The same set of weights is applied accross all time steps, making RNNs efficient for sequential data. \n",
    "3. Sequential processing: Data is processed one time step at a time, making RNNs ideal for tasks where order matters. \n",
    "\n",
    "## Application\n",
    "1. Natural language processing\n",
    "2. Time series analysis\n",
    "3. Speech Recognition\n",
    "4. Music Generation\n",
    "\n",
    "# Sequence To Vector\n",
    "\n",
    "- Is a model paradigm in which a sequential input, is processed and mapped to a fixed-sized vector representation. This approach condeses all the relevant information from the sequence into a single feature vector, which can then be usaed for tasks such as classificication, regression, or further downstream processing. \n",
    "\n",
    "## Key features:\n",
    "\n",
    "1. Hidden state: RNNS maintain a hidden states that gets updated at each time step. This hidden state acts as a memory, storing information about precious elements in the sequence. \n",
    "2. Shared weights: Teh same set of weights is applied accros all time steps, makind RNNs efficient for sequential data. \n",
    "3. Sequential processing: Data is processed one time step at a time, making RNNs idea for tasks where order matters. \n",
    "\n",
    "## Application \n",
    "\n",
    "1. Natural language processing\n",
    "2. Time series analysis\n",
    "3. Music generation \n",
    "\n",
    "# LSTM\n",
    "\n",
    "- Long Short-Term Memory: Is a special type of RNN designed to address the limitations of traditional RNNs, particularly the vanishing gradient problem, which prevents standard RNNs from learning long-term dependencies in sequences. \n",
    "\n",
    "## Key features of LSTM:\n",
    "\n",
    "1. Memory Cell: The core of an LSTM is its memory cell, which allows it to retain information over long sequences. The memory cell is controlled by three gates:\n",
    "- Forget gate: Decides what information from the cell state to forget. \n",
    "- Input gate: Determines which new information to update in the cell state. \n",
    "- Output gate: Controls how much of the cell's state to pass to the next time step.\n",
    "\n",
    "2. Cell state and hidden state:\n",
    "- The cell state acts as the \"long-term memory\" and carries information across time steps with minimal modification. \n",
    "- The hidden state acts as the \"short term memory\" and is outputted at each time step.\n",
    "\n",
    "3. Learnable mechanism: By learning when to store, update and discard information, LSTMs can focus on the most relevant parts of a sequence while ignoring irrelevant parts. \n",
    "\n",
    "## Advantages\n",
    "- LSTMs excel at capturing long-term dependencies in sequential data. \n",
    "- They effectively handle sequences with varying lenghts and complex temporal patterns. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
